{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch_geometric torch_sparse torch_scatter torch_spline_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch_geometric\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pickle\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "from sklearn.metrics import accuracy_score,recall_score, average_precision_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = 5\n",
    "features = 26\n",
    "batch_size = 3\n",
    "seq_len = 10   # 序列的长度\n",
    "train_rate = 0.67"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "指标信息： (8640, 130)\n",
      "边集: torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "DATASET = \"DatasetUpdate/MBD (1).csv\"\n",
    "TOPOLOGY = \"DatasetUpdate/MBD_topology.pk\"\n",
    "\n",
    "data = pd.read_csv(DATASET, header=[0,1])\n",
    "# preprocess\n",
    "labels = data['label']\n",
    "metric = data.drop(['date', 'label'], axis = 1)\n",
    "metric.columns.names = ['host','metric']\n",
    "tempm = metric.swaplevel('metric','host',axis=1).stack()\n",
    "\n",
    "tempm = (tempm-tempm.mean())/(tempm.std())\n",
    "metric = tempm.unstack().swaplevel('metric','host',axis=1).stack().unstack()\n",
    "\n",
    "with open(TOPOLOGY, 'rb') as f:\n",
    "    edge_tensor = pickle.load(f)\n",
    "    \n",
    "data = metric.values\n",
    "edge_tensor = np.array(edge_tensor)\n",
    "edge_tensor = torch.LongTensor(edge_tensor)\n",
    "print(\"指标信息：\",data.shape)\n",
    "print(\"边集:\",edge_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 整理数据格式\n",
    "由于我们的模型输入必须是一个序列，所以我们必须把数据整理为(数据总数，序列长度，实际数据)的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8640, 5, 26)\n",
      "序列数据形状: (8631, 10, 5, 26)\n"
     ]
    }
   ],
   "source": [
    "data_reshape = data.reshape((-1,nodes,features))\n",
    "print(data_reshape.shape)\n",
    "def sequence_data_preparation(seq_len, data):\n",
    "    X = []\n",
    "    for i in range(data.shape[0] - int(seq_len - 1) ):\n",
    "        X.append(data[i : i + seq_len, ...])\n",
    "    return np.array(X)\n",
    "\n",
    "data_sequence  = sequence_data_preparation(seq_len, data_reshape)\n",
    "print(\"序列数据形状:\",data_sequence.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3划分数据集\n",
    "数据划分按照2/3训练集和1/3测试集进行划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:  (5782, 10, 5, 26)\n",
      "Test data:  (2849, 10, 5, 26)\n"
     ]
    }
   ],
   "source": [
    "def train_test_split(data, train_portion):\n",
    "    time_len = data.shape[0]\n",
    "    train_size = int(time_len * train_portion)\n",
    "    train_data = np.array(data[:train_size,...])\n",
    "    test_data = np.array(data[train_size:,...])\n",
    "    return train_data, test_data\n",
    "\n",
    "train_data, test_data = train_test_split(data_sequence,train_rate)\n",
    "print(\"Train data: \", train_data.shape)\n",
    "print(\"Test data: \", test_data.shape)\n",
    "train_data_tensor = torch.Tensor(train_data)\n",
    "test_data_tensor = torch.Tensor(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "丢弃训练样本数量: 1\n",
      "丢弃测试样本数量: 2\n"
     ]
    }
   ],
   "source": [
    "train_data_tensor_batch = torch.utils.data.DataLoader(train_data_tensor,batch_size=batch_size,drop_last=True)\n",
    "test_data_tensor_batch = torch.utils.data.DataLoader(test_data_tensor,batch_size=batch_size,drop_last=True)\n",
    "train_size = int(train_data_tensor.shape[0] / batch_size)\n",
    "test_size = int(test_data_tensor.shape[0] / batch_size)\n",
    "print(\"丢弃训练样本数量:\",train_data_tensor.shape[0] - train_size * batch_size)\n",
    "print(\"丢弃测试样本数量:\",test_data_tensor.shape[0] - test_size * batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建TopoMAD模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphLSTM(torch.nn.Module):\n",
    "    def __init__(self,batch_size,edge_tensor,nodes,features,sequtienal,lstm_output):\n",
    "        super(GraphLSTM, self).__init__()\n",
    "        \"\"\"\n",
    "        需要指定的输入: [sequtienal, batch, nodes, features]\n",
    "        GCN的输出：  [nodes, 30]\n",
    "        LSTM的输出： [5 × self.nodes]\n",
    "        模型最终输出: [batch , 5 * self.nodes]\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.nodes = nodes\n",
    "        self.input_features_size = features\n",
    "        self.sequtienal = sequtienal\n",
    "        self.edge_tensor = edge_tensor\n",
    "\n",
    "        lstm_output = lstm_output\n",
    "        gcn_output = int(lstm_output * 1.3)\n",
    "        lstm_input_size = gcn_output * self.nodes\n",
    "        lstm_output_flatten = lstm_output * self.nodes\n",
    "\n",
    "        gcn_input = self.input_features_size + lstm_output\n",
    "        \n",
    "        self.gcn_encoder = GCNConv(gcn_input,gcn_output)\n",
    "        self.h = torch.zeros(batch_size, lstm_output_flatten)\n",
    "        self.c = torch.zeros(batch_size, lstm_output_flatten)\n",
    "        self.cell = nn.LSTMCell(lstm_input_size, lstm_output_flatten)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h, c = self.h, self.c\n",
    "        h_list = []\n",
    "        for xt in x:\n",
    "            h = h.reshape(self.batch_size,self.nodes,-1)\n",
    "            xxt = torch.cat([xt,h],dim=2)\n",
    "            xxt = self.gcn_encoder(xxt,self.edge_tensor)\n",
    "            xxt = xxt.view(self.batch_size,-1)\n",
    "            h = h.view(self.batch_size,-1)\n",
    "            h, c = self.cell(xxt, (h, c))\n",
    "            h_list.append(h)\n",
    "        h_list = torch.stack(h_list,1)\n",
    "        h_list = h_list.view(self.batch_size,self.sequtienal,self.nodes,-1)\n",
    "        h_list = h_list.transpose(0,1)\n",
    "        return h, c, xt, h_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self,batch_size,input_size,output_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        \"\"\"\n",
    "        需要指定的输入: [batch, input_size]\n",
    "        模型最终输出: [batch , output_size]\n",
    "        \"\"\"\n",
    "        self.encoder_log_var = nn.Linear(input_size,output_size)\n",
    "        self.encoder_mu = nn.Linear(input_size,output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = F.sigmoid(x)\n",
    "        return self.encoder_mu(h), self.encoder_log_var(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphLSTM_Decoder(torch.nn.Module):\n",
    "    def __init__(self,decoder,batch_size,edge_tensor,nodes,features,sequtienal,lstm_output):\n",
    "        super(GraphLSTM_Decoder, self).__init__()\n",
    "        \"\"\"\n",
    "        需要指定的输入: [sequtienal, batch, nodes, features]\n",
    "        GCN的输出：  [nodes, 30]\n",
    "        LSTM的输出： [5 × self.nodes]\n",
    "        模型最终输出: [batch , 5 * self.nodes]\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.nodes = nodes\n",
    "        self.input_features_size = features\n",
    "        self.sequtienal = sequtienal\n",
    "        self.edge_tensor = edge_tensor\n",
    "        \n",
    "        gcn_output = int(lstm_output * 1.3)\n",
    "        lstm_output = lstm_output\n",
    "        lstm_input_size = gcn_output * self.nodes\n",
    "        lstm_output_flatten = lstm_output * self.nodes\n",
    "\n",
    "        gcn_input = self.input_features_size + lstm_output\n",
    "        \n",
    "        self.gcn = GCNConv(gcn_input,gcn_output)\n",
    "        self.h = torch.zeros(batch_size, lstm_output_flatten)\n",
    "        self.cell = nn.LSTMCell(lstm_input_size, lstm_output_flatten)\n",
    "    \n",
    "        self.decoder = decoder\n",
    "        self.forcing = True\n",
    "        \n",
    "\n",
    "    def forward(self, x, c,reconst_mu,reconst_mu_list, reonst_log_var_list, origin):\n",
    "        reconst_h = self.h\n",
    "        \n",
    "\n",
    "        for xt in torch.flip(x[:-1],dims=[0]):\n",
    "            reconst_h = reconst_h.reshape(self.batch_size,self.nodes,-1)\n",
    "\n",
    "            if self.forcing:\n",
    "                xxt = torch.cat([xt,reconst_h],dim=2)\n",
    "            else:\n",
    "                reconst_mu = reconst_mu.view(self.batch_size,self.nodes,-1)\n",
    "                xxt = torch.cat([reconst_mu,reconst_h],dim=2)\n",
    "            \n",
    "            xxt = self.gcn(xxt,self.edge_tensor)\n",
    "            xxt = xxt.view(self.batch_size,-1)\n",
    "            reconst_h = reconst_h.view(self.batch_size,-1)\n",
    "            reconst_h, c = self.cell(xxt, (reconst_h, c))\n",
    "\n",
    "            # Reconst: torch.Size([3, 100])\n",
    "            reconst_mu, reonst_log_var = self.decoder(reconst_h)  \n",
    "\n",
    "            reconst_mu_list.append(reconst_mu)\n",
    "            reonst_log_var_list.append(reonst_log_var)\n",
    "            origin.append(xt)\n",
    "            \n",
    "        tran_view = (self.batch_size, self.sequtienal,self.nodes, -1)\n",
    "        \n",
    "        reconst_mu_list = torch.stack(reconst_mu_list).transpose(0,1)\n",
    "        reconst_mu_list = reconst_mu_list.view(*tran_view)\n",
    "\n",
    "        reonst_log_var_list = torch.stack(reonst_log_var_list,0).transpose(0,1)\n",
    "        reonst_log_var_list = reonst_log_var_list.view(*tran_view)\n",
    "        origin = torch.stack(origin,0).transpose(0,1)\n",
    "        origin = origin.view(*tran_view)\n",
    "        return reconst_mu_list,reonst_log_var_list,origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopoMAD(torch.nn.Module):\n",
    "    def __init__(self,batch_size,edge_tensor,nodes,features,sequtienal):\n",
    "        super(TopoMAD, self).__init__()\n",
    "        # encoder GraphLSTM_1\n",
    "        self.batch_size = batch_size\n",
    "        self.nodes = nodes\n",
    "        lstm_01_output = 15\n",
    "        \n",
    "        ness = (batch_size,edge_tensor,nodes,features,sequtienal)\n",
    "        self.graph_01 = GraphLSTM(*ness,lstm_01_output)\n",
    "        \n",
    "        latent_size = 15\n",
    "        self.encoder = Encoder(batch_size,lstm_01_output * self.nodes, latent_size)\n",
    "        \n",
    "        hidden_size = lstm_01_output * self.nodes\n",
    "        self.z_mlp = nn.Linear(latent_size,hidden_size)\n",
    "        \n",
    "        self.decoder = Encoder(batch_size,hidden_size, self.nodes * features)  # 60 -> 5 * 26\n",
    "        self.GraphLSTM_Decoder = GraphLSTM_Decoder(self.decoder,*ness,lstm_01_output)\n",
    "        self.forcing = False\n",
    "        self.sequtienal = sequtienal\n",
    "    \n",
    "    def decode(self, x):\n",
    "        h = F.sigmoid(x)\n",
    "        return self.decoder_mu(h), self.decoder_log_var(h)\n",
    " \n",
    "    def reparamterize(self, mu, log_var):\n",
    "        std = torch.exp(log_var / 2)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h, c, xt, h_list = self.graph_01(x)  # H   [10,3,5,26] -> [3, 5 * 20]\n",
    "        mu, log_var = self.encoder(h)        # mu: [3,100] -> [3,15]\n",
    "        z = self.reparamterize(mu, log_var)  # z:  [3,15]\n",
    "        z_point = self.z_mlp(z)              #     [3,15] -> [3,100]\n",
    "        reconst_mu, reonst_log_var = self.decoder(z_point)  # [3,100] -> [3, 130]\n",
    "        reconst_mu_list, reonst_log_var_list, origin = [reconst_mu], [reonst_log_var], [xt]\n",
    "        reconst_mu_list, reonst_log_var_list, origin = self.GraphLSTM_Decoder(x,c,reconst_mu,reconst_mu_list, reonst_log_var_list, origin)\n",
    "        \n",
    "        mu = mu.view(self.batch_size,self.nodes,-1)\n",
    "        log_var = log_var.view(self.batch_size,self.nodes,-1)\n",
    "        \n",
    "        return reconst_mu_list,origin, mu, log_var, reonst_log_var_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(preds, origin, mu, logvar, output_logvar):\n",
    "    recon_loss = 0.5 * torch.mean(torch.sum(torch.div((preds - origin) ** 2, output_logvar.exp()) + output_logvar, (1,2,3)))\n",
    "    s = torch.sum(1 + logvar - mu**2 - logvar.exp(),(1,2))\n",
    "    kl_loss = -0.5 * torch.mean(s)\n",
    "    total_loss = recon_loss + kl_loss\n",
    "    return total_loss, recon_loss, kl_loss\n",
    "\n",
    "def is_anomaly(preds, origin, output_logvar): \n",
    "    div = torch.div((preds - origin) ** 2, output_logvar.exp())\n",
    "    s = torch.sum( div + output_logvar, (2,3))\n",
    "    recon_loss = 0.5 * torch.mean(s,1)\n",
    "    return recon_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "topomad = TopoMAD(batch_size,edge_tensor,nodes,features,seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(topomad.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 1, Process: [1920/1927], kl_div: 1.3732, reconst_loss: -284.4203\n",
      " Process: [900/949]\n",
      "AP: 0.25956149263406886\n",
      " Epoch: 2, Process: [1440/1927], kl_div: 0.7337, reconst_loss: -355.6284"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch += 1\n",
    "    random = np.random.random()\n",
    "    if random < 0.2:\n",
    "        topomad.forcing = False\n",
    "    else:\n",
    "        topomad.forcing = True\n",
    "        \n",
    "    for id_,xt in enumerate(train_data_tensor_batch):\n",
    "        xt = xt.transpose(0,1)  # 10, 3, 5, 26\n",
    "        reconst_mu,origin, mu, log_var, reonst_log_var_list = topomad(xt)\n",
    "        total_loss, recon_loss, kl_loss = loss_function(reconst_mu,origin, mu, log_var, reonst_log_var_list)\n",
    "\n",
    "        if id_ % 30 == 0:\n",
    "            print(\"\\r\",\"Epoch: {}, Process: [{}/{}], kl_div: {:.4f}, reconst_loss: {:.4f}\".format(epoch,id_,train_size,kl_loss,recon_loss),end=\"\",flush=True)\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print()\n",
    "    topomad.forcing = False\n",
    "    n = test_data_tensor.shape[0]\n",
    "    pred_result = []\n",
    "    for id_,x in enumerate(test_data_tensor_batch):\n",
    "        with torch.no_grad():\n",
    "            x = x.transpose(0,1)\n",
    "            reconst_mu,origin, mu, log_var, reonst_log_var_list = topomad(x)\n",
    "            t = is_anomaly(reconst_mu, origin, reonst_log_var_list)\n",
    "            pred_result.append(t.detach().numpy())\n",
    "            if id_ % 50 == 0:\n",
    "                print(\"\\r\",\"Test Process: [{}/{}]\".format(id_,test_size),end=\"\",flush=True)\n",
    "\n",
    "    score_array = np.hstack(pred_result)\n",
    "    n = score_array.shape[0]\n",
    "\n",
    "    labels_array = labels.values.flatten()\n",
    "    test_labels = labels_array[:-9][-n:]\n",
    "    ap = average_precision_score(test_labels,score_array)\n",
    "    print()\n",
    "    print(\"Test AP:\",ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
